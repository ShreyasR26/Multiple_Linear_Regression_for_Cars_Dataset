"""**************************************************************************************************************************************
Projet Name: Multiple Linear Regression for a given data set about car sales.

Description: A given data set,Car_Prices.csv has certain details for various cars. These include brand, price, Audi, Mileage,
             Engine Value, Engine Type, Registration, Year and Model.The aim is to find out how each of these parameters affects 
             'Price' of the car. Many steps need to be taken into account. 
                  1. Downloading the Data
                     Using Pandas libraries, Car_Prices.csv can be downloaded and the data can be read and described.
                  2. Cleaning the data
                     This involves many steps including:
                         a). Checking for high variablity
                         b). Checking for missing values
                         c). Checking for Skewness and removing outliers.
                         d). Checking for incorrect values.
                  3. Checking for non-linearity in the data and making it linear it using logrithms from numpy libraries.
                  4. Checking for multi-collinearity between the columns and removing that using variance inflation
                     factor from statsmodels. Any variable with vif>>6 has to be removed.
                  5. Getting Dummy Variables: Using pandas librabries to create n-1 dummy variables for n columns.
                  6. Scalig the data using statmodels and getting it ready for R2.
                  7.Train the model using Linear Regression
                  8. Testing the model to make predictions.
                 
********************************************************************************************************************************************"""


"""******************************************************************************************************************************************
Date: 7 July 2020

Name: Shreyas Ramani

*********************************************************************************************************************************************"""



"""********************************************************************************************************************************************"""
#Imprted Libraries

import numpy as np
#numpy is used for creating various arrays for calculations

import pandas as pd
#pandas is used for reading csv files and creating dummy variables

import matplotlib.pyplot as plt
#used for showing various plots

import seaborn as sb
sb.set()
#seaborn is used for getting various data plots.

from statsmodels.stats.outliers_influence import variance_inflation_factor
#To check multicollineaality between the variables

from sklearn.preprocessing import StandardScaler
#used for scaling

from sklearn.model_selection import train_test_split
#Used to verify the model

from sklearn.linear_model import LinearRegression
#To perform linear regression
"""******************************************************************************************************************************"""


"""********************************************************************************************************************************"""
#Reading The Data

raw_data=pd.read_csv('C:/Users/admin/Downloads/Car Prices.csv')
#Reads the csv file(spreadsheet) from the pandas library.

print(raw_data)
#displays the raw data from the csv file

print(raw_data.describe(include='all'))
#displays the statistics for all data available
"""**********************************************************************************************************************************"""


"""**********************************************************************************************************************************"""
# 2 a) Checking for high variability

data=raw_data.drop(['Model'],axis=1)
#The column 'Model' is being dropped because there are too many different models(312)
#axis=0 for rows, axis=1 for columns.

print(data)
#displays data after 'Model' is removed

print(data.describe(include='all'))
"""**********************************************************************************************************************************"""


"""**********************************************************************************************************************************"""
#2 b) Checking for missing Values

print(data.isnull())
#prints the missing values

print(data.isnull().sum())
#gives a list of missing values, 172 in price and 150 in EngineV

data_no_mv=data.dropna(axis=0)
#removing all missing values in the data

print(data_no_mv.isnull().sum())
#gives final result of no missing values


"""**********************************************************************************************************************************"""



"""**********************************************************************************************************************************"""
#2 c) Checking For skewness

sb.distplot(data_no_mv['Price'])
#due to excess outliers in the price, we are removing the top 1%
q=data_no_mv['Price'].quantile(0.99)
#Taking the 99th percentile of Price

data_1=data_no_mv[data_no_mv['Price']<q]
#displaying everything less than the 99th percentile of price
print(data_1.describe(include='all'))

sb.distplot(data_1['Mileage'])
#mileage too has its outliers.

q=data_1['Mileage'].quantile(0.99)
#Taking the 99th percentile of Mileage

data_2=data_no_mv[data_no_mv['Mileage']<q]
#displaying everything less than the 99th percentile of price
print(data_2.describe(include='all'))

data_3=data_2[data_2['EngineV']<6.5]
#gives approprite value to Engine Value

q=data_3['Year'].quantile(0.01)
#skewed on the right side, so removing outliers from the left
data_4=data_3[data_3['Year']>q]

sb.distplot(data_4['Year'])

data_cleaned=data_4.reset_index(drop=True)
#Final cleaned data, dropping unnessary content.


"""**********************************************************************************************************************************"""


"""**********************************************************************************************************************************"""
"""Multi-collinearallity"""

log_price=np.log(data_cleaned['Price'])
#Converting the cleaned data to a log form.

data_cleaned['Log_Price']=log_price
#adding log price to table
data_cleaned=data_cleaned.drop(['Price'],axis=1)
#removing Price column
print(data_cleaned.columns.values)
#display the table contents

variables=data_cleaned[['Mileage','Year','EngineV']]
#columns with potential multicollineararity

vif=pd.DataFrame()
vif["VIF"]=[variance_inflation_factor(variables.values,i) for i in range(variables.shape[1])]
#
vif["Features"] = variables.columns
print(vif)
data_multicollinearity=data_cleaned.drop(['Year'],axis=1)
#vif value should ideally be between 1 and 5, way too high for year,10.317

"""**********************************************************************************************************************************"""


"""**********************************************************************************************************************************"""
"""Creating Dummy Variables"""

data_with_dummies=pd.get_dummies(data_multicollinearity,drop_first='True')
#adding dummy variables to everything except the first one
print(data_with_dummies.head())

targets=data_with_dummies['Log_Price']

inputs=data_with_dummies.drop(['Log_Price'],axis=1)

"""**********************************************************************************************************************************"""


"""**********************************************************************************************************************************"""
"""Scaling the Data"""

scaler=StandardScaler()

scaler.fit(inputs)

inputs_scaled=scaler.transform(inputs)

"""Trans_Train_Split"""
x_train, x_test , y_train , y_test =train_test_split(inputs_scaled, targets,test_size=0.2,random_state=365)


reg = LinearRegression()
#perform linear Regression
reg.fit(x_train,y_train)

print(reg.score(x_train,y_train))
#find R2 value

y_hat=reg.predict(x_train)
#for model validation

"""For plotting data"""
plt.scatter(y_train,y_hat)
plt.xlabel('Targets',fontsize=20)
plt.ylabel('Predictions',fontsize=20)
plt.xlim(6,13)
#for scale of plot
plt.ylim(6,13)

plt.show()

sb.distplot(y_train-y_hat)

print(reg.intercept_)
print(reg.coef_)

reg_summary=pd.DataFrame(inputs.columns.values,columns=['Features'])

reg_summary['weights']=reg.coef_
print(reg_summary)

"""**********************************************************************************************************************************"""


"""Testing The model"""

y_hat_test=reg.predict(x_test)

plt.scatter(y_test,y_hat_test,alpha=0.2)
plt.xlabel('y_test',fontsize=20)
plt.ylabel('y_hat',fontsize=20)
plt.xlim(6,13)
plt.ylim(6,13)
plt.show()

df_pf=pd.DataFrame(np.exp(y_hat_test),columns=['Predictions'])
print(df_pf.head())
#dataframe performance

y_test=y_test.reset_index(drop=True)
print(y_test.head())


df_pf['Target']=np.exp(y_test)
print(df_pf)



df_pf['Residual']=df_pf['Target']-df_pf['Predictions']

df_pf['Difference']=np.absolute(df_pf['Residual']/df_pf['Target']*100)
print(df_pf)

print(df_pf.describe())
#final list gives us various statistics needed for final prediction

"""**********************************************************************************************************************************"""

"""**********************************************************************************************************************************"""
